# -*- coding: utf-8 -*-
"""MLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XcJ3N0TlAkx-Z4QhGyLHzyBwxLbx2P2p
"""

import numpy as np
import pdb
import os
from tqdm import tqdm
import sys

from matplotlib import pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader

import torchvision
from torchvision.datasets import FashionMNIST
from torchvision import transforms


# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyper-parameters 
input_size = 10
hidden_size = 100
num_classes = 4
num_epochs =10
learning_rate = 0.001

#other Parameter
train_start=201
train_end=1001
validate_start=101
validate_end=201
train_size=60000
test_size=10000

#Train Data Loader
trans_img = transforms.Compose([transforms.ToTensor()])
train_dataset = FashionMNIST("./data/", train=True, transform=trans_img, download=True)
trainloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)

#Test data loader
test_dataset = FashionMNIST("./data/", train=False, transform=trans_img, download=True)
testloader = DataLoader(test_dataset, batch_size=1024, shuffle=False)

print(test_dataset)

# Fully connected neural network with one hidden layer
class MLP(nn.Module):
    def __init__(self, num_classes=10):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(784, 256) 
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128 , 64)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 16)
        self.cln = nn.Linear(16 ,10)

    
    def forward(self, x):
        x = x.view(-1,784) 
        x = F.tanh(self.fc1(x))
        x = F.tanh(self.fc2(x))
        x = F.tanh(self.fc3(x))
        x = F.tanh(self.fc4(x))
        x = F.tanh(self.fc5(x))
        x = F.log_softmax(self.cln(x), dim=1)
        return x

model = MLP(10).to(device)
# print(model.parameters)

# Loss and optimizer
criterion = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) 

#Train the model
num_epochs=30

loss_vec = []
for i in tqdm(range(num_epochs)):
    model.train()
    avg_loss=0
    j=0
    for batch_idx, (img, target) in enumerate(trainloader):
        img = Variable(img).to(device)
        target = Variable(target).to(device)

        # Zero out the gradients
        optimizer.zero_grad()

        # Forward Propagation
        out = model(img)
        loss = F.cross_entropy(out, target)

        # backward propagation
        loss.backward()
        avg_loss+=loss
        j=j+1
        # Update the model parameters
        optimizer.step()
    avg_loss=avg_loss/j;
    print(avg_loss)
    loss_vec.append(avg_loss)

plt.figure()
plt.plot(loss_vec)
plt.title("training-loss-MLP")
plt.savefig("./images/training_mlp.jpg")

torch.save(model.state_dict(), "./models/MLP.pt")

#evaluation of model
model.eval()
avg_loss = 0
j=0

y_gt = []
y_pred_label = []

for batch_idx, (img, y_true) in enumerate(testloader):
    img = Variable(img).to(device)
    y_true = Variable(y_true).to(device)
    y_gt.append(y_true)
    out = model(img)
    y_pred_label_tmp = torch.argmax(out, dim=1)
    y_pred_label.append(y_pred_label_tmp)
    loss = F.cross_entropy(out, y_true)
    print(loss.item())
    avg_loss+=loss.item()
    j=j+1
print(avg_loss/j)

correct=0
for i in range(10):
  for j in range(len(y_gt[i])):
    if y_gt[i][j].item()==y_pred_label[i][j].item():
      correct+=1
print(correct/10000)

